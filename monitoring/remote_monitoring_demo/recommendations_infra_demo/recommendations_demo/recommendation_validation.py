"""
Copyright (c) 2023, 2023 Red Hat, IBM Corporation and others.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

import pandas as pd
import json
import csv
import sys
import os
import datetime
import getopt
import subprocess
import filecmp
import shutil
import subprocess
import math

# Validate the recommendations generated to the csv created by scripts(which contains the recommendation logic)
def validate_recomm(filename):
    with open(filename, 'r') as f:
       data = json.load(f)
    df = pd.read_csv('recommendation_check.csv')
    # Compare the data for each time zone
    for json_data in data:
        for kubernetes_object in json_data[0]["kubernetes_objects"]:
            for container in kubernetes_object["containers"]:
                for time_zone in container["recommendations"]["data"]:
                    for duration_type in container["recommendations"]["data"][time_zone]["duration_based"]:
                        recommendations = container["recommendations"]["data"][time_zone]
                        if "config" in recommendations["duration_based"][duration_type]:
                            cpu_limits_json = round(recommendations["duration_based"][duration_type]["config"]["limits"]["cpu"]["amount"], 4)
                            memory_limits_json = round(recommendations["duration_based"][duration_type]["config"]["limits"]["memory"]["amount"], 4)
                            cpu_requests_json = round(recommendations["duration_based"][duration_type]["config"]["requests"]["cpu"]["amount"], 4)
                            memory_requests_json = round(recommendations["duration_based"][duration_type]["config"]["requests"]["memory"]["amount"], 4)
                            
                            
                            # Compare the CPU and memory values with the corresponding values in the CSV file
                            csv_row = df.loc[(df["time_zone"] == time_zone) & (df["term"] == duration_type)]
                            if len(csv_row) > 0:
                                cpu_limits_csv = round(csv_row["cpu_limits"].values[0], 4)
                                memory_limits_csv = round(csv_row["memory_limits"].values[0], 4)
                                cpu_requests_csv = round(csv_row["cpu_requests"].values[0], 4)
                                memory_requests_csv = round(csv_row["memory_requests"].values[0], 4)
                                
                                if cpu_limits_json == cpu_limits_csv and memory_limits_json == memory_limits_csv and cpu_requests_json == cpu_requests_csv and memory_requests_json == memory_requests_csv :
                                    print(f"Match found for timezone {time_zone} and duration type {duration_type}")
                            else:
                                print(f"No match found for timezone {time_zone} and duration type {duration_type}")

# Validate the recommendations and boxplots generated by Kruize by comparing with existing files
def validate_experiment_recommendations_boxplots_actuals(exp_name, exp_type, inputfile, validatefile, match_type):
    validate_files = filecmp.cmp(inputfile, validatefile)
    if validate_files:
        print(match_type, " MATCH for ", exp_type , " experiment ", exp_name)
        exit(0)
    else:
        print(match_type, " DOESN'T MATCH for ", exp_type, " experiment ", exp_name)
        exit(1)

def round_values(value, precision=5):
    """Round the value to a specified precision if it is numeric."""
    if isinstance(value, (int, float)):
        return round(value, precision)
    return value

def validate_experiment_recommendations_boxplots(exp_name, exp_type, inputfile, validatefile, match_type, precision=5):
    validate_files = filecmp.cmp(inputfile, validatefile)
    if validate_files:
        print(match_type, " MATCH for ", exp_type , " experiment ", exp_name)
        exit(0)
    else:
        df1 = pd.read_csv(inputfile)
        df2 = pd.read_csv(validatefile)

        if df1.shape != df2.shape:
            print(match_type, " DOESN'T MATCH for ", exp_type , " experiment ", exp_name)
            exit(1)

        differences_found = False
        for idx, row1 in df1.iterrows():
            row2 = df2.iloc[idx]

            differing_columns = []

            for col in df1.columns:
                val1 = row1[col]
                val2 = row2[col]

                # If both values are numeric (and not NaN), round them and compare
                if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
                    # Round both values
                    val1_rounded = round_values(val1, precision)
                    val2_rounded = round_values(val2, precision)

                    # Compare the rounded values, ignore NaN
                    if not math.isnan(val1_rounded) and not math.isnan(val2_rounded) and val1_rounded != val2_rounded:
                        differing_columns.append(col)

            if differing_columns:
                differences_found = True

        if not differences_found:
            print(match_type, " MATCH for ", exp_type , " experiment ", exp_name)
            exit(0)
        else:
            print(match_type, " DOESN'T MATCH for ", exp_type , " experiment ", exp_name)
            exit(1)


def getUniquek8Objects(inputcsvfile):
    column_name = 'k8ObjectName'
    # Read the CSV file and get the unique values of the specified column
    unique_values = set()
    
    with open(inputcsvfile, 'r') as csv_file:
        csv_reader = csv.DictReader(csv_file)
        for row in csv_reader:
            unique_values.add(row[column_name])
    return unique_values


def aggregateWorkloads(filename, outputResults):

    print("Aggregating the data for file : ", filename)

    # Load the CSV file into a pandas DataFrame
    df = pd.read_csv(filename)

    #Remove the rows if there is no owner_kind, owner_name and workload
    # Expected to ignore rows which can be pods / invalid
    columns_to_check = ['owner_kind', 'owner_name', 'workload', 'workload_type']
    df = df.dropna(subset=columns_to_check, how='any')

    # Ignore rows with 'workload_type' value of 'job'
    df = df[df['workload_type'] != 'job']

    # Create a column with k8_object_type
    # Based on the data observed, these are the assumptions:
    # If owner_kind is 'ReplicaSet' and workload is '<none>', actual workload_type is ReplicaSet
    # If owner_kind is 'ReplicationCOntroller' and workload is '<none>', actual workload_type is ReplicationController
    # If owner_kind and workload has some names, workload_type is same as derived through queries.

    df['k8_object_type'] = ''
    for i, row in df.iterrows():
        if row['owner_kind'] == 'ReplicaSet' and row['workload'] == '<none>':
            df.at[i, 'k8_object_type'] = 'replicaset'
        elif row['owner_kind'] == 'ReplicationController' and row['workload'] == '<none>':
            df.at[i, 'k8_object_type'] = 'replicationcontroller'
        else:
            df.at[i, 'k8_object_type'] = row['workload_type']

    # Update k8_object_name based on the type and workload.
    # If the workload is <none> (which indicates ReplicaSet and ReplicationCOntroller - ignoring pods/invalid cases), the name of the k8_object can be owner_name.
    # If the workload has some other name, the k8_object_name is same as workload. In this case, owner_name cannot be used as there can be multiple owner_names for the same deployment(considering there are multiple replicasets)

    df['k8_object_name'] = ''
    for i, row in df.iterrows():
        if row['workload'] != '<none>':
            df.at[i, 'k8_object_name'] = row['workload']
        else:
            df.at[i, 'k8_object_name'] = row['owner_name']

    df.to_csv('cop-withobjType.csv', index=False)

    # Specify the columns to sort by
    # Sort and grpup the data based on below columns to get a container for a workload and for an interval.
    # Each file generated is for a single timestamp and a container for a workload and will be aggregated to a single metrics value.
    sort_columns = ['namespace', 'k8_object_type', 'workload', 'container_name', 'interval_start']
    sorted_df = df.sort_values(sort_columns)

    # Group the rows by the unique values
    grouped = sorted_df.groupby(sort_columns)

    # Create a directory to store the output CSV files
    output_dir = 'output'
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Write each group to a separate CSV file
    counter = 0
    for key, group in grouped:
        counter += 1
        filename = f"file_{counter}.csv"
        filepath = os.path.join(output_dir, filename)
        group.to_csv(filepath, index=False)

    #Create a temporary file with a header to append the aggregate data from multiple files.
    header_row = df.columns.tolist()
    agg_df = pd.DataFrame(columns=header_row)
    columns_to_ignore = ['pod', 'owner_name', 'node']
    if 'resource_id' in df.columns:
        columns_to_ignore.append('resource_id')

    for filename in os.listdir(output_dir):
        if filename.endswith('.csv'):
            filepath = os.path.join(output_dir, filename)
            df = pd.read_csv(filepath)

            # Calculate the average and minimum values for specific columns
            for column in df.columns:
                if column.endswith('avg'):
                    avg = df[column].mean()
                    df[column] = avg
                elif column.endswith('min'):
                    minimum = df[column].min()
                    df[column] = minimum
                elif column.endswith('max'):
                    maximum = df[column].max()
                    df[column] = maximum
                elif column.endswith('sum'):
                    total = df[column].sum()
                    df[column] = total

            df = df.drop_duplicates(subset=[col for col in df.columns if col not in columns_to_ignore])
            #agg_df = agg_df.append(df)
            agg_df = pd.concat([agg_df, df], ignore_index=True)

    agg_df.to_csv('./final.csv', index=False)
    df1 = pd.read_csv('final.csv')
    df1.drop(columns_to_ignore, axis=1, inplace=True)
    df1.to_csv(outputResults, index=False)
    os.remove('final.csv')


def convert_date_format(input_date_str):

    DATE_FORMATS = ["%a %b %d %H:%M:%S %Z %Y", "%Y-%m-%dT%H:%M:%S.%f", "%a %b %d %H:%M:%S UTC %Y", "%Y-%m-%d %H:%M:%S %Z", "%Y-%m-%d %H:%M:%S %z %Z"]

    for date_format in DATE_FORMATS:
        try:
            dt = datetime.datetime.strptime(input_date_str, date_format)
            dt_utc = dt.astimezone(datetime.timezone.utc)
            output_date_str = dt_utc.strftime("%Y-%m-%dT%H:%M:%S.000Z")
            return output_date_str
        except ValueError:
            continue
    raise ValueError(f"Unrecognized date format: {input_date_str} ")

# Create results json for namespace experiment from csv
def create_namespace_json_from_csv(csv_file_path, outputjsonfile):

    # Check if output file already exists. If yes, delete that.
    ## TODO: Recheck if this is necessary
    if os.path.exists(outputjsonfile):
        os.remove(outputjsonfile)

    # Define the list that will hold the final JSON data
    json_data = []

    # Create an empty list to hold the deployments
    deployments = []
    mebibyte = 1048576

    with open(csv_file_path, 'r') as csvfile:
        csvreader = csv.DictReader(csvfile)

        for row in csvreader:
            namespace_metrics_all = {}
            namespace_metrics = []

    ## Hardcoding for tfb-results and demo benchmark. Updating them only if these columns are not available.
        ## Keep this until the metrics queries are fixed in benchmark to get the below column data
            columns_tocheck = [ "namespace" , "cluster_name" ]
            image_name = "kruize/tfb-qrh:2.9.1.F"
            container_name = "tfb-server"
            k8_object_type = "deployment"
            k8_object_name = "tfb-qrh-sample-0"
            namespace = "tfb-perf"
            cluster_name = "e23-alias"

            for col in columns_tocheck:
                if col not in row:
                    if col == "image_name":
                        row[col] = image_name
                    elif col == "container_name":
                        row[col] = container_name
                    elif col == "k8_object_type":
                        row[col] = k8_object_type
                    elif col == "k8_object_name":
                        row[col] = k8_object_name
                    elif col == "namespace":
                        row[col] = namespace
                    elif col == "cluster_name":
                        row[col] = cluster_name

            if row["cpu_request_namespace_sum"]:
                namespace_metrics.append({
            "name": "namespaceCpuRequest",
                        "results": {
                            "aggregation_info": {
                                "sum": float(row["cpu_request_namespace_sum"]),
                                "format": "cores"
                                }
                            }
            })
            if row["cpu_limit_namespace_sum"]:
                namespace_metrics.append({
            "name" : "namespaceCpuLimit",
                        "results": {
                            "aggregation_info": {
                                "sum": float(row["cpu_limit_namespace_sum"]),
                                "format": "cores"
                                }
                            }
                        })
            if row["cpu_throttle_namespace_min"] and row["cpu_throttle_namespace_max"]:
                namespace_metrics.append({
            "name" : "namespaceCpuThrottle",
                        "results": {
                            "aggregation_info": {
                                "min": float(row["cpu_throttle_namespace_min"]),
                                "max": float(row["cpu_throttle_namespace_max"]),
                                "avg": float(row["cpu_throttle_namespace_avg"]),
                                "format": "cores"
                                }
                            }
                        })
            elif row["cpu_throttle_namespace_max"]:
                namespace_metrics.append({
            "name" : "namespaceCpuThrottle",
                        "results": {
                            "aggregation_info": {
                                "max": float(row["cpu_throttle_namespace_max"]),
                                "avg": float(row["cpu_throttle_namespace_avg"]),
                                "format": "cores"
                                }
                            }
                        })

            if row["cpu_usage_namespace_avg"]:
                namespace_metrics.append({
                    "name" : "namespaceCpuUsage",
                    "results": {
                        "aggregation_info": {
                            "min": float(row["cpu_usage_namespace_min"]),
                            "max": float(row["cpu_usage_namespace_max"]),
                            "avg": float(row["cpu_usage_namespace_avg"]),
                            "format": "cores"
                            }
                        }
                    })
            if row["memory_request_namespace_sum"]:
                namespace_metrics.append({
            "name" : "namespaceMemoryRequest",
                        "results": {
                            "aggregation_info": {
                                "sum": float(row["memory_request_namespace_sum"])/mebibyte,
                                "format": "MiB"
                                }
                            }
                        })
            if row["memory_limit_namespace_sum"]:
                namespace_metrics.append({
            "name" : "namespaceMemoryLimit",
                        "results": {
                            "aggregation_info": {
                                "sum": float(row["memory_limit_namespace_sum"])/mebibyte,
                                "format": "MiB"
                                }
                            }
                        })
            if row["memory_usage_namespace_avg"]:
                namespace_metrics.append({
                        "name" : "namespaceMemoryUsage",
                        "results": {
                            "aggregation_info": {
                                "min": float(row["memory_usage_namespace_min"])/mebibyte,
                                "max": float(row["memory_usage_namespace_max"])/mebibyte,
                                "avg": float(row["memory_usage_namespace_avg"])/mebibyte,
                                "format": "MiB"
                                }
                            }
                        })
            if row["memory_rss_usage_namespace_avg"]:
                namespace_metrics.append({
                        "name" : "namespaceMemoryRSS",
                        "results": {
                            "aggregation_info": {
                                "min": float(row["memory_rss_usage_namespace_min"])/mebibyte,
                                "max": float(row["memory_rss_usage_namespace_max"])/mebibyte,
                                "avg": float(row["memory_rss_usage_namespace_avg"])/mebibyte,
                                "format": "MiB"
                                }
                            }
                        })

            # Create a dictionary to hold the container information
            namespace = {
                "namespace": row["namespace"],
                "metrics": namespace_metrics
            }

            # Create a list to hold the containers
            namespaces = [namespace]

            # Create a dictionary to hold the deployment information
            kubernetes_object = {
                "namespaces": namespace
            }
            kubernetes_objects = [kubernetes_object]

            # Create a dictionary to hold the experiment data
            experiment = {
                "version": "v2.0",
                "experiment_name": row["cluster_name"] + '|' + row["namespace"],
                "interval_start_time": convert_date_format(row["start_timestamp"]),
                "interval_end_time": convert_date_format(row["end_timestamp"]),
                "kubernetes_objects": kubernetes_objects
            }

            json_data.append(experiment)
    with open(outputjsonfile, "w") as json_file:
        json.dump(json_data, json_file)

# Create results json for container experiment from csv
def create_json_from_csv(csv_file_path, outputjsonfile):

    # Check if output file already exists. If yes, delete that.
    ## TODO: Recheck if this is necessary
    if os.path.exists(outputjsonfile):
        os.remove(outputjsonfile)

    # Define the list that will hold the final JSON data
    json_data = []

    # Create an empty list to hold the deployments
    deployments = []
    mebibyte = 1048576

    with open(csv_file_path, 'r') as csvfile:
        csvreader = csv.DictReader(csvfile)

        for row in csvreader:
            container_metrics_all = {}
            container_metrics = []

	## Hardcoding for tfb-results and demo benchmark. Updating them only if these columns are not available.
        ## Keep this until the metrics queries are fixed in benchmark to get the below column data
            columns_tocheck = [ "image_name" , "container_name" , "k8_object_type" , "k8_object_name" , "namespace" , "cluster_name" ]
            image_name = "kruize/tfb-qrh:2.9.1.F"
            container_name = "tfb-server"
            k8_object_type = "deployment"
            k8_object_name = "tfb-qrh-sample-0"
            namespace = "tfb-perf"
            cluster_name = "e23-alias"

            for col in columns_tocheck:
                if col not in row:
                    if col == "image_name":
                        row[col] = image_name
                    elif col == "container_name":
                        row[col] = container_name
                    elif col == "k8_object_type":
                        row[col] = k8_object_type
                    elif col == "k8_object_name":
                        row[col] = k8_object_name
                    elif col == "namespace":
                        row[col] = namespace
                    elif col == "cluster_name":
                        row[col] = cluster_name

            if row["cpu_request_container_avg"]:
                container_metrics.append({
			"name": "cpuRequest",
                        "results": {
                            "aggregation_info": {
                                "sum": float(row["cpu_request_container_sum"]),
                                "avg": float(row["cpu_request_container_avg"]),
                                "format": "cores"
                                }
                            }
			})
            if row["cpu_limit_container_avg"]:
                container_metrics.append({
			"name" : "cpuLimit",
                        "results": {
                            "aggregation_info": {
                                "sum": float(row["cpu_limit_container_sum"]),
                                "avg": float(row["cpu_limit_container_avg"]),
                                "format": "cores"
                                }
                            }
                        })
            if row["cpu_throttle_container_max"]:
                container_metrics.append({
			"name" : "cpuThrottle",
                        "results": {
                            "aggregation_info": {
                                "sum": float(row["cpu_throttle_container_sum"]),
                                "max": float(row["cpu_throttle_container_max"]),
                                "avg": float(row["cpu_throttle_container_avg"]),
                                "format": "cores"
                                }
                            }
                        })
            if row["cpu_usage_container_avg"]:
                container_metrics.append({
                    "name" : "cpuUsage",
                    "results": {
                        "aggregation_info": {
                            "sum": float(row["cpu_usage_container_sum"]),
                            "min": float(row["cpu_usage_container_min"]),
                            "max": float(row["cpu_usage_container_max"]),
                            "avg": float(row["cpu_usage_container_avg"]),
                            "format": "cores"
                            }
                        }
                    })
            if row["memory_request_container_avg"]:
                container_metrics.append({
			"name" : "memoryRequest",
                        "results": {
                            "aggregation_info": {
                                "sum": float(row["memory_request_container_sum"])/mebibyte,
                                "avg": float(row["memory_request_container_avg"])/mebibyte,
                                "format": "MiB"
                                }
                            }
                        })
            if row["memory_limit_container_avg"]:
                container_metrics.append({
			"name" : "memoryLimit",
                        "results": {
                            "aggregation_info": {
                                "sum": float(row["memory_limit_container_sum"])/mebibyte,
                                "avg": float(row["memory_limit_container_avg"])/mebibyte,
                                "format": "MiB"
                                }
                            }
                        })
            if row["memory_usage_container_avg"]:
                container_metrics.append({
                        "name" : "memoryUsage",
                        "results": {
                            "aggregation_info": {
                                "min": float(row["memory_usage_container_min"])/mebibyte,
                                "max": float(row["memory_usage_container_max"])/mebibyte,
                                "sum": float(row["memory_usage_container_sum"])/mebibyte,
                                "avg": float(row["memory_usage_container_avg"])/mebibyte,
                                "format": "MiB"
                                }
                            }
                        })   
            if row["memory_usage_container_avg"]:
                container_metrics.append({
                        "name" : "memoryRSS",
                        "results": {
                            "aggregation_info": {
                                "min": float(row["memory_rss_usage_container_min"])/mebibyte,
                                "max": float(row["memory_rss_usage_container_max"])/mebibyte,
                                "sum": float(row["memory_rss_usage_container_sum"])/mebibyte,
                                "avg": float(row["memory_rss_usage_container_avg"])/mebibyte,
                                "format": "MiB"
                                }
                            }
                        })
            if "accelerator_core_usage_percentage_max" in row and row["accelerator_core_usage_percentage_max"]:
                if "node" in row and row["node"]:
                    container_metrics.append({
                        "name" : "acceleratorCoreUsage",
                        "results": {
                            "metadata": {
                                "accelerator_model_name": row["accelerator_model_name"],
                                "node": row["node"]
                            },
                            "aggregation_info": {
                                "min": float(row["accelerator_core_usage_percentage_min"]),
                                "max": float(row["accelerator_core_usage_percentage_max"]),
                                "avg": float(row["accelerator_core_usage_percentage_avg"]),
                                "format": "percentage"
                                }
                            }
                        })
                else:
                    container_metrics.append({
                        "name" : "acceleratorCoreUsage",
                        "results": {
                            "metadata": {
                                "accelerator_model_name": row["accelerator_model_name"]
                            },
                            "aggregation_info": {
                                "min": float(row["accelerator_core_usage_percentage_min"]),
                                "max": float(row["accelerator_core_usage_percentage_max"]),
                                "avg": float(row["accelerator_core_usage_percentage_avg"]),
                                "format": "percentage"
                                }
                            }
                        })

            if "accelerator_memory_copy_percentage_max" in row and row["accelerator_memory_copy_percentage_max"]:
                if "node" in row and row["node"]:
                    container_metrics.append({
                        "name" : "acceleratorMemoryUsage",
                        "results": {
                            "metadata": {
                                "accelerator_model_name": row["accelerator_model_name"],
                                "node": row["node"]
                            },
                            "aggregation_info": {
                                "min": float(row["accelerator_memory_copy_percentage_min"]),
                                "max": float(row["accelerator_memory_copy_percentage_max"]),
                                "avg": float(row["accelerator_memory_copy_percentage_avg"]),
                                "format": "percentage"
                                }
                            }
                        })
                else:
                    container_metrics.append({
                        "name" : "acceleratorMemoryUsage",
                        "results": {
                            "metadata": {
                                "accelerator_model_name": row["accelerator_model_name"]
                            },
                            "aggregation_info": {
                                "min": float(row["accelerator_memory_copy_percentage_min"]),
                                "max": float(row["accelerator_memory_copy_percentage_max"]),
                                "avg": float(row["accelerator_memory_copy_percentage_avg"]),
                                "format": "percentage"
                                }
                            }
                        })
            if "accelerator_frame_buffer_usage_max" in row and row["accelerator_frame_buffer_usage_max"]:
                if "node" in row and row["node"]:
                    container_metrics.append({
                        "name" : "acceleratorFrameBufferUsage",
                        "results": {
                            "metadata": {
                                "accelerator_model_name": row["accelerator_model_name"],
                                "node": row["node"]
                            },
                            "aggregation_info": {
                                "min": float(row["accelerator_frame_buffer_usage_min"]),
                                "max": float(row["accelerator_frame_buffer_usage_max"]),
                                "avg": float(row["accelerator_frame_buffer_usage_avg"]),
                                "format": "percentage"
                                }
                            }
                        })
                else:
                    container_metrics.append({
                        "name" : "acceleratorFrameBufferUsage",
                        "results": {
                            "metadata": {
                                "accelerator_model_name": row["accelerator_model_name"]
                            },
                            "aggregation_info": {
                                "min": float(row["accelerator_frame_buffer_usage_min"]),
                                "max": float(row["accelerator_frame_buffer_usage_max"]),
                                "avg": float(row["accelerator_frame_buffer_usage_avg"]),
                                "format": "percentage"
                                }
                            }
                        })


            # Create a dictionary to hold the container information
            container = {
                "container_image_name": row["image_name"],
                "container_name": row["container_name"],
                "metrics": container_metrics
            }

            # Create a list to hold the containers
            containers = [container]

            # Create a dictionary to hold the deployment information
            kubernetes_object = {
                "type": row["k8_object_type"],
                "name": row["k8_object_name"],
                "namespace": row["namespace"],
                "containers": containers
            }
            kubernetes_objects = [kubernetes_object]

            # Create a dictionary to hold the experiment data
            experiment = {
                "version": "1.0",
                "experiment_name": row["container_name"] + '|' + row["k8_object_name"] + '|' + row["k8_object_type"] + '|' + row["namespace"] + '|' + row["cluster_name"],
                "interval_start_time": convert_date_format(row["start_timestamp"]),
                "interval_end_time": convert_date_format(row["end_timestamp"]),
                "kubernetes_objects": kubernetes_objects
            }

            json_data.append(experiment)

    with open(outputjsonfile, "w") as json_file:
        json.dump(json_data, json_file)

## Get the metrics and recommendations data from listExperiments for namespace experiment_type
def getNamespaceExperimentMetrics(filename):
    with open(filename, 'r') as f:
      data = json.load(f)
    if not data:
        print("No experiments found!")
    else:
        with open('experimentMetrics_temp.csv', 'w', newline='') as f:
            fieldnames = ['experiment_name', 'namespace', 'timezone', 'namespaceCpuRequest_sum', 'namespaceCpuLimit_sum', 'namespaceMemoryRequest_sum', 'namespaceMemoryLimit_sum', 'namespaceCpuUsage_avg', 'namespaceCpuUsage_min', 'namespaceCpuUsage_max',  'namespaceCpuThrottle_avg', 'namespaceCpuThrottle_min', 'namespaceCpuThrottle_max', 'namespaceMemoryRSS_avg',  'namespaceMemoryRSS_min', 'namespaceMemoryRSS_max', 'namespaceMemoryUsage_avg',  'namespaceMemoryUsage_min', 'namespaceMemoryUsage_max', 'cost_short_term_cpu_requests', 'cost_short_term_memory_requests', 'cost_short_term_cpu_limits', 'cost_short_term_memory_limits', 'cost_medium_term_cpu_requests', 'cost_medium_term_memory_requests', 'cost_medium_term_cpu_limits', 'cost_medium_term_memory_limits', 'cost_long_term_cpu_requests', 'cost_long_term_memory_requests', 'cost_long_term_cpu_limits', 'cost_long_term_memory_limits', 'cost_short_term_cpu_requests_variation', 'cost_short_term_memory_requests_variation', 'cost_short_term_cpu_limits_variation', 'cost_short_term_memory_limits_variation', 'cost_medium_term_cpu_requests_variation' , 'cost_medium_term_memory_requests_variation', 'cost_medium_term_cpu_limits_variation' , 'cost_medium_term_memory_limits_variation', 'cost_long_term_cpu_requests_variation' , 'cost_long_term_memory_requests_variation', 'cost_long_term_cpu_limits_variation' , 'cost_long_term_memory_limits_variation', 'performance_short_term_cpu_requests', 'performance_short_term_memory_requests', 'performance_short_term_cpu_limits', 'performance_short_term_memory_limits', 'performance_medium_term_cpu_requests', 'performance_medium_term_memory_requests', 'performance_medium_term_cpu_limits', 'performance_medium_term_memory_limits', 'performance_long_term_cpu_requests', 'performance_long_term_memory_requests', 'performance_long_term_cpu_limits', 'performance_long_term_memory_limits', 'performance_short_term_cpu_requests_variation', 'performance_short_term_memory_requests_variation', 'performance_short_term_cpu_limits_variation', 'performance_short_term_memory_limits_variation', 'performance_medium_term_cpu_requests_variation' , 'performance_medium_term_memory_requests_variation', 'performance_medium_term_cpu_limits_variation' , 'performance_medium_term_memory_limits_variation', 'performance_long_term_cpu_requests_variation' , 'performance_long_term_memory_requests_variation', 'performance_long_term_cpu_limits_variation' , 'performance_long_term_memory_limits_variation', 'namespaceCpuUsage_format', 'namespaceMemoryUsage_format']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()

            datadict = data[0]
            for key, value in datadict.items():
                if key == "experiment_name":
                    experiment_name = value
                if key == "kubernetes_objects":
                    for kobj in value:
                        namespaceNames = []
                        namespace = kobj["namespace"]
                        for namespace_name,namespace_data in kobj["namespaces"].items():
                            namespaceName = namespace_data["namespace"]
                            namespaceNames.append(namespaceName)
                            if "results" in namespace_data:
                                for timezone, timezone_data in namespace_data["results"].items():
                                    kobj_dict = {
                                            'experiment_name': experiment_name,
                                            'namespace': kobj["namespace"],
                                            'timezone': timezone
                                            }
                                    metric_dict = {}
                                    recomm_dict = {}
                                    for metric_name, metric_data in timezone_data["metrics"].items():
                                        for agg_name, agg_value in metric_data["aggregation_info"].items():
                                            metric_agg_var_name = metric_name + '_' + agg_name
                                            if agg_name != "format":
                                                metric_dict[metric_agg_var_name] = str(agg_value)
                                            elif metric_agg_var_name == "namespaceCpuUsage_format" or metric_agg_var_name == "namespaceMemoryUsage_format":
                                                metric_dict[metric_agg_var_name] = str(agg_value)
                                    for recomm_timezone, recomm_data in namespace_data["recommendations"]["data"].items():
                                        if recomm_timezone == timezone:
                                            for recomm_type, recomm_typedata in recomm_data["recommendation_terms"].items():
                                                if 'recommendation_engines' in recomm_typedata:
                                                    for recomm_engine, recomm_enginedata in recomm_typedata["recommendation_engines"].items():
                                                        if "config" in recomm_enginedata:
                                                            for recomm_config, recomm_configmetrics in recomm_enginedata["config"].items():
                                                                for recomm_resource, recomm_resourcedata in recomm_configmetrics.items():
                                                                    recomm_var_name = recomm_engine + '_' + recomm_type + '_' + recomm_resource + '_' + recomm_config
                                                                    recomm_dict[recomm_var_name] = str(recomm_resourcedata["amount"])
                                                        if "variation" in recomm_enginedata:
                                                            for recomm_config, recomm_configmetrics in recomm_enginedata["variation"].items():
                                                                for recomm_resource, recomm_resourcedata in recomm_configmetrics.items():
                                                                    recomm_var_name = recomm_engine + '_' + recomm_type + '_' + recomm_resource + '_' + recomm_config + '_variation'
                                                                    recomm_dict[recomm_var_name] = str(recomm_resourcedata["amount"])
                                    kobj_dict.update(metric_dict)
                                    kobj_dict.update(recomm_dict)
                                    writer.writerow(kobj_dict)
                            else:
                                kobj_dict = {
                                            'experiment_name': experiment_name,
                                            'namespace': kobj["namespace"]
                                            }
                                writer.writerow(kobj_dict)

        # Sort the data in chronological order of timezone
        with open('experimentMetrics_temp.csv', 'r') as csvfile:
            reader = csv.DictReader(csvfile)
            Edata = list(reader)
        data_sorted = sorted(Edata, key=lambda x: x['timezone'])
        with open('experimentMetrics_sorted.csv', 'w', newline='') as csvfile:
            fieldnames = reader.fieldnames
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data_sorted)

        # Write the sorted data back to the CSV file
        with open('experimentOutput.csv', 'a', newline='') as csvfile:
            fieldnames = reader.fieldnames
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data_sorted)


## Get the metrics and recommendations data from listExperiments
def getExperimentMetrics(filename):
    with open(filename, 'r') as f:
      data = json.load(f)
    if not data:
        print("No experiments found!")
    else:
        with open('experimentMetrics_temp.csv', 'w', newline='') as f:
            datadict = data[0]
            containsGPUdata=False

            ## Check if it has GPU recommendations
            for key, value in datadict.items():
                if key == "kubernetes_objects":
                    for kobj in value:
                        for container_name,container_data in kobj["containers"].items():
                            if "results" in container_data:
                                for timezone, timezone_data in container_data["results"].items():
                                    for metric_name, metric_data in timezone_data["metrics"].items():
                                        if metric_name in ["acceleratorCoreUsage", "acceleratorMemoryUsage", "acceleratorFrameBufferUsage"]:
                                            containsGPUdata=True
                                            break

            if containsGPUdata == True:
                fieldnames = ['experiment_name', 'namespace', 'type', 'name', 'container_name', 'timezone', 'cpuUsage_sum', 'cpuUsage_avg', 'cpuUsage_max', 'cpuUsage_min', 'cpuThrottle_sum', 'cpuThrottle_avg', 'cpuThrottle_max', 'cpuRequest_sum', 'cpuRequest_avg', 'cpuLimit_sum', 'cpuLimit_avg', 'memoryRSS_sum', 'memoryRSS_avg', 'memoryRSS_max', 'memoryRSS_min', 'memoryUsage_sum', 'memoryUsage_avg', 'memoryUsage_max', 'memoryUsage_min', 'memoryRequest_sum', 'memoryRequest_avg',  'memoryLimit_sum', 'memoryLimit_avg', 'acceleratorFrameBufferUsage_min', 'acceleratorFrameBufferUsage_max', 'acceleratorFrameBufferUsage_avg', 'acceleratorMemoryUsage_min', 'acceleratorMemoryUsage_max', 'acceleratorMemoryUsage_avg', 'acceleratorCoreUsage_min', 'acceleratorCoreUsage_max', 'acceleratorCoreUsage_avg', 'cost_short_term_cpu_requests', 'cost_short_term_memory_requests', 'cost_short_term_cpu_limits', 'cost_short_term_memory_limits', 'cost_short_term_gpu_limits', 'cost_medium_term_cpu_requests', 'cost_medium_term_memory_requests', 'cost_medium_term_cpu_limits', 'cost_medium_term_memory_limits', 'cost_medium_term_gpu_limits', 'cost_long_term_cpu_requests', 'cost_long_term_memory_requests', 'cost_long_term_cpu_limits', 'cost_long_term_memory_limits', 'cost_long_term_gpu_limits', 'cost_short_term_cpu_requests_variation', 'cost_short_term_memory_requests_variation', 'cost_short_term_cpu_limits_variation', 'cost_short_term_memory_limits_variation', 'cost_medium_term_cpu_requests_variation' , 'cost_medium_term_memory_requests_variation', 'cost_medium_term_cpu_limits_variation' , 'cost_medium_term_memory_limits_variation', 'cost_long_term_cpu_requests_variation' , 'cost_long_term_memory_requests_variation', 'cost_long_term_cpu_limits_variation' , 'cost_long_term_memory_limits_variation', 'performance_short_term_cpu_requests', 'performance_short_term_memory_requests', 'performance_short_term_cpu_limits', 'performance_short_term_memory_limits', 'performance_short_term_gpu_limits', 'performance_medium_term_cpu_requests', 'performance_medium_term_memory_requests', 'performance_medium_term_cpu_limits', 'performance_medium_term_memory_limits', 'performance_medium_term_gpu_limits', 'performance_long_term_cpu_requests', 'performance_long_term_memory_requests', 'performance_long_term_cpu_limits', 'performance_long_term_memory_limits', 'performance_long_term_gpu_limits', 'performance_short_term_cpu_requests_variation', 'performance_short_term_memory_requests_variation', 'performance_short_term_cpu_limits_variation', 'performance_short_term_memory_limits_variation', 'performance_medium_term_cpu_requests_variation' , 'performance_medium_term_memory_requests_variation', 'performance_medium_term_cpu_limits_variation' , 'performance_medium_term_memory_limits_variation', 'performance_long_term_cpu_requests_variation' , 'performance_long_term_memory_requests_variation', 'performance_long_term_cpu_limits_variation' , 'performance_long_term_memory_limits_variation', 'cpuUsage_format', 'memoryUsage_format']
            else:
                fieldnames = ['experiment_name', 'namespace', 'type', 'name', 'container_name', 'timezone', 'cpuUsage_sum', 'cpuUsage_avg', 'cpuUsage_max', 'cpuUsage_min', 'cpuThrottle_sum', 'cpuThrottle_avg', 'cpuThrottle_max', 'cpuRequest_sum', 'cpuRequest_avg', 'cpuLimit_sum', 'cpuLimit_avg', 'memoryRSS_sum', 'memoryRSS_avg', 'memoryRSS_max', 'memoryRSS_min', 'memoryUsage_sum', 'memoryUsage_avg', 'memoryUsage_max', 'memoryUsage_min', 'memoryRequest_sum', 'memoryRequest_avg',  'memoryLimit_sum', 'memoryLimit_avg', 'cost_short_term_cpu_requests', 'cost_short_term_memory_requests', 'cost_short_term_cpu_limits', 'cost_short_term_memory_limits', 'cost_medium_term_cpu_requests', 'cost_medium_term_memory_requests', 'cost_medium_term_cpu_limits', 'cost_medium_term_memory_limits', 'cost_long_term_cpu_requests', 'cost_long_term_memory_requests', 'cost_long_term_cpu_limits', 'cost_long_term_memory_limits', 'cost_short_term_cpu_requests_variation', 'cost_short_term_memory_requests_variation', 'cost_short_term_cpu_limits_variation', 'cost_short_term_memory_limits_variation', 'cost_medium_term_cpu_requests_variation' , 'cost_medium_term_memory_requests_variation', 'cost_medium_term_cpu_limits_variation' , 'cost_medium_term_memory_limits_variation', 'cost_long_term_cpu_requests_variation' , 'cost_long_term_memory_requests_variation', 'cost_long_term_cpu_limits_variation' , 'cost_long_term_memory_limits_variation', 'performance_short_term_cpu_requests', 'performance_short_term_memory_requests', 'performance_short_term_cpu_limits', 'performance_short_term_memory_limits', 'performance_medium_term_cpu_requests', 'performance_medium_term_memory_requests', 'performance_medium_term_cpu_limits', 'performance_medium_term_memory_limits', 'performance_long_term_cpu_requests', 'performance_long_term_memory_requests', 'performance_long_term_cpu_limits', 'performance_long_term_memory_limits', 'performance_short_term_cpu_requests_variation', 'performance_short_term_memory_requests_variation', 'performance_short_term_cpu_limits_variation', 'performance_short_term_memory_limits_variation', 'performance_medium_term_cpu_requests_variation' , 'performance_medium_term_memory_requests_variation', 'performance_medium_term_cpu_limits_variation' , 'performance_medium_term_memory_limits_variation', 'performance_long_term_cpu_requests_variation' , 'performance_long_term_memory_requests_variation', 'performance_long_term_cpu_limits_variation' , 'performance_long_term_memory_limits_variation', 'cpuUsage_format', 'memoryUsage_format']

            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()


            for key, value in datadict.items():
                if key == "experiment_name":
                    experiment_name = value
                if key == "kubernetes_objects":
                    for kobj in value:
                        containerNames=[]
                        k8ObjectName = ""
                        namespace = ""
                        k8ObjectType = ""
                        k8ObjectName = kobj["name"]
                        namespace = kobj["namespace"]
                        k8ObjectType = kobj["type"]
                        for container_name,container_data in kobj["containers"].items():
                            containerName = container_data["container_name"]
                            containerNames.append(containerName)
                            if "results" in container_data:
                                for timezone, timezone_data in container_data["results"].items():
                                    kobj_dict = {
                                            'experiment_name': experiment_name,
                                            'type': kobj["type"],
                                            'name': kobj["name"],
                                            'namespace': kobj["namespace"],
                                            'container_name': container_data["container_name"],
                                            'timezone': timezone,
                                            }
                                    metric_dict = {}
                                    recomm_dict = {}
                                    for metric_name, metric_data in timezone_data["metrics"].items():
                                        for agg_name, agg_value in metric_data["aggregation_info"].items():
                                            metric_agg_var_name = metric_name + '_' + agg_name
                                            if agg_name != "format":
                                                metric_dict[metric_agg_var_name] = str(agg_value)
                                            elif metric_agg_var_name == "cpuUsage_format" or metric_agg_var_name == "memoryUsage_format":
                                                metric_dict[metric_agg_var_name] = str(agg_value)
                                    for recomm_timezone, recomm_data in container_data["recommendations"]["data"].items():
                                        if recomm_timezone == timezone:
                                            for recomm_type, recomm_typedata in recomm_data["recommendation_terms"].items():
                                                if 'recommendation_engines' in recomm_typedata:
                                                    for recomm_engine, recomm_enginedata in recomm_typedata["recommendation_engines"].items():
                                                        if "config" in recomm_enginedata:
                                                            for recomm_config, recomm_configmetrics in recomm_enginedata["config"].items():
                                                                for recomm_resource, recomm_resourcedata in recomm_configmetrics.items():
                                                                    if recomm_resource not in ["cpu", "memory"]:
                                                                        recomm_var_name = recomm_engine + '_' + recomm_type + '_gpu_' + recomm_config
                                                                        recomm_dict[recomm_var_name] = str(recomm_resource)
                                                                    else:
                                                                        recomm_var_name = recomm_engine + '_' + recomm_type + '_' + recomm_resource + '_' + recomm_config
                                                                        recomm_dict[recomm_var_name] = str(recomm_resourcedata["amount"])
                                                        if "variation" in recomm_enginedata:
                                                            for recomm_config, recomm_configmetrics in recomm_enginedata["variation"].items():
                                                                for recomm_resource, recomm_resourcedata in recomm_configmetrics.items():
                                                                    recomm_var_name = recomm_engine + '_' + recomm_type + '_' + recomm_resource + '_' + recomm_config + '_variation'
                                                                    recomm_dict[recomm_var_name] = str(recomm_resourcedata["amount"])
                                    kobj_dict.update(metric_dict)
                                    kobj_dict.update(recomm_dict)
                                    writer.writerow(kobj_dict)
                            else:
                                kobj_dict = {
                                            'experiment_name': experiment_name,
                                            'type': kobj["type"],
                                            'name': kobj["name"],
                                            'namespace': kobj["namespace"],
                                            'container_name': container_data["container_name"],
                                            }
                                writer.writerow(kobj_dict)
                                
        # Sort the data in chronological order of timezone
        with open('experimentMetrics_temp.csv', 'r') as csvfile:
            reader = csv.DictReader(csvfile)
            Edata = list(reader)
        data_sorted = sorted(Edata, key=lambda x: x['timezone'])
        with open('experimentMetrics_sorted.csv', 'w', newline='') as csvfile:
            fieldnames = reader.fieldnames
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data_sorted)

        # Write the sorted data back to the CSV file
        with open('experimentOutput.csv', 'a', newline='') as csvfile:
            fieldnames = reader.fieldnames
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data_sorted)


def get_recommondations(filename):
    with open(filename, 'r') as f:
      data = json.load(f)
    if not data:
        print("No experiments found!")
    else:
        with open('experimentRecommendations_temp.csv', 'w', newline='') as f:
            fieldnames = ['experiment_name', 'cluster_name', 'namespace', 'type', 'name', 'container_name', 'timezone', 'current_cpu_requests', 'current_memory_requests', 'current_cpu_limits', 'current_memory_limits', 'cost_short_term_cpu_requests', 'cost_short_term_memory_requests', 'cost_short_term_cpu_limits', 'cost_short_term_memory_limits', 'cost_medium_term_cpu_requests', 'cost_medium_term_memory_requests', 'cost_medium_term_cpu_limits', 'cost_medium_term_memory_limits', 'cost_long_term_cpu_requests', 'cost_long_term_memory_requests', 'cost_long_term_cpu_limits', 'cost_long_term_memory_limits', 'cost_short_term_cpu_requests_variation', 'cost_short_term_memory_requests_variation', 'cost_short_term_cpu_limits_variation', 'cost_short_term_memory_limits_variation', 'cost_medium_term_cpu_requests_variation' , 'cost_medium_term_memory_requests_variation', 'cost_medium_term_cpu_limits_variation' , 'cost_medium_term_memory_limits_variation', 'cost_long_term_cpu_requests_variation' , 'cost_long_term_memory_requests_variation', 'cost_long_term_cpu_limits_variation' , 'cost_long_term_memory_limits_variation', 'performance_short_term_cpu_requests', 'performance_short_term_memory_requests', 'performance_short_term_cpu_limits', 'performance_short_term_memory_limits', 'performance_medium_term_cpu_requests', 'performance_medium_term_memory_requests', 'performance_medium_term_cpu_limits', 'performance_medium_term_memory_limits', 'performance_long_term_cpu_requests', 'performance_long_term_memory_requests', 'performance_long_term_cpu_limits', 'performance_long_term_memory_limits', 'performance_short_term_cpu_requests_variation', 'performance_short_term_memory_requests_variation', 'performance_short_term_cpu_limits_variation', 'performance_short_term_memory_limits_variation', 'performance_medium_term_cpu_requests_variation' , 'performance_medium_term_memory_requests_variation', 'performance_medium_term_cpu_limits_variation' , 'performance_medium_term_memory_limits_variation', 'performance_long_term_cpu_requests_variation' , 'performance_long_term_memory_requests_variation', 'performance_long_term_cpu_limits_variation' , 'performance_long_term_memory_limits_variation', 'cost_short_term_cpu_requests_format', 'cost_short_term_memory_requests_format', 'cost_short_term_cpu_limits_format', 'cost_short_term_memory_limits_format', 'cost_medium_term_cpu_requests_format' , 'cost_medium_term_memory_requests_format', 'cost_medium_term_cpu_limits_format' , 'cost_medium_term_memory_limits_format', 'cost_long_term_cpu_requests_format' , 'cost_long_term_memory_requests_format', 'cost_long_term_cpu_limits_format' , 'cost_long_term_memory_limits_format' ]

            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            experiment_name = ""
            datadict = data[0]

            experiment_name = datadict["experiment_name"]
            for key, value in datadict.items():
                if key == "experiment_name":
                    print("experiment_name = ", value)
                    experiment_name = value
                if key == "cluster_name":
                    cluster_name = value
                if key == "kubernetes_objects":
                    for kobj in value:
                        containerNames=[]
                        k8ObjectName = ""
                        namespace = ""
                        k8ObjectType = ""
                        k8ObjectName = kobj["name"]
                        namespace = kobj["namespace"]
                        k8ObjectType = kobj["type"]
                        for index, container_data in enumerate(kobj["containers"]):
                            containerName = container_data.get("container_name")
                            containerNames.append(containerName)
                            containerData = container_data.get("recommendations", {}).get("data", {})
                            if not containerData:
                                kobj_dict = {
                                'experiment_name': experiment_name,
                                'type': kobj["type"],
                                'name': kobj["name"],
                                'namespace': kobj["namespace"],
                                'container_name': container_data["container_name"],
                                }
                                writer.writerow(kobj_dict)
                            for timezone, timezone_data in containerData.items():
                                kobj_dict = {
                                'experiment_name': experiment_name,
                                'cluster_name': cluster_name,
                                'type': kobj["type"],
                                'name': kobj["name"],
                                'namespace': kobj["namespace"],
                                'container_name': container_data["container_name"],
                                'timezone': timezone,
                                }
                                recomm_dict = {}
                                for current_resource, current_resourcedata in timezone_data["current"].items():
                                    if 'requests' in current_resourcedata:
                                        if 'cpu' in current_resourcedata["requests"]:
                                            recomm_dict["current_cpu_requests"] = str(current_resourcedata["requests"]["cpu"]["amount"])
                                        if 'memory' in current_resourcedata["requests"]:
                                            recomm_dict["current_memory_requests"] = str(current_resourcedata["requests"]["memory"]["amount"])
                                    if 'limits' in current_resourcedata:
                                        if 'cpu' in current_resourcedata["limits"]:
                                            recomm_dict["current_cpu_limits"] = str(current_resourcedata["limits"]["cpu"]["amount"])
                                        if 'memory' in current_resourcedata["limits"]:
                                            recomm_dict["current_memory_limits"] = str(current_resourcedata["limits"]["memory"]["amount"])


                                for recomm_type, recomm_typedata in timezone_data["recommendation_terms"].items():
                                    if 'recommendation_engines' in recomm_typedata:
                                        for recomm_engine, recomm_enginedata in recomm_typedata["recommendation_engines"].items():
                                            if "config" in recomm_enginedata:
                                                for recomm_config, recomm_configmetrics in recomm_enginedata["config"].items():
                                                    for recomm_resource, recomm_resourcedata in recomm_configmetrics.items():
                                                        recomm_var_name = recomm_engine + '_' + recomm_type + '_' + recomm_resource + '_' + recomm_config
                                                        recomm_var_format = recomm_engine + '_' + recomm_type + '_' + recomm_resource + '_' + recomm_config + '_format'
                                                        recomm_dict[recomm_var_name] = str(recomm_resourcedata["amount"])
                                                        if recomm_engine != "performance":
                                                            recomm_dict[recomm_var_format] = str(recomm_resourcedata["format"])
                                            if "variation" in recomm_enginedata:
                                                for recomm_config, recomm_configmetrics in recomm_enginedata["variation"].items():
                                                    for recomm_resource, recomm_resourcedata in recomm_configmetrics.items():
                                                        recomm_var_name = recomm_engine + '_' + recomm_type + '_' + recomm_resource + '_' + recomm_config +  '_variation'
                                                        recomm_dict[recomm_var_name] = str(recomm_resourcedata["amount"])
                                kobj_dict.update(recomm_dict)
                                writer.writerow(kobj_dict)
        # Sort the data in chronological order of timezone
        with open('experimentRecommendations_temp.csv', 'r') as csvfile:
            reader = csv.DictReader(csvfile)
            Edata = list(reader)
        data_sorted = sorted(Edata, key=lambda x: x['timezone'])
        
        with open('experimentRecommendations_sorted_temp.csv', 'w', newline='') as csvfile:
            fieldnames = reader.fieldnames
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data_sorted)

        # Write the sorted data back to the CSV file
        with open('experimentRecommendations.csv', 'a', newline='') as csvfile:
            fieldnames = reader.fieldnames
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data_sorted)

        os.remove('experimentRecommendations_temp.csv')
        os.remove('experimentRecommendations_sorted_temp.csv')

def getExperimentBoxPlots(filename):
    with open(filename, 'r') as f:
      data = json.load(f)
    if not data:
        print("No experiments found!")
    else:
        with open('experimentPlotData_temp.csv', 'w', newline='') as f:
            fieldnames = ['cluster_name', 'experiment_name', 'type', 'name', 'namespace', 'container_name', 'timezone', 'short_term_tz1' , 'st_tz1_cpu_min', 'st_tz1_cpu_q1', 'st_tz1_cpu_median', 'st_tz1_cpu_q3', 'st_tz1_cpu_max', 'st_tz1_mem_min', 'st_tz1_mem_q1', 'st_tz1_mem_median', 'st_tz1_mem_q3', 'st_tz1_mem_max', 'short_term_tz2', 'st_tz2_cpu_min', 'st_tz2_cpu_q1', 'st_tz2_cpu_median', 'st_tz2_cpu_q3', 'st_tz2_cpu_max', 'st_tz2_mem_min', 'st_tz2_mem_q1', 'st_tz2_mem_median', 'st_tz2_mem_q3', 'st_tz2_mem_max', 'short_term_tz3', 'st_tz3_cpu_min', 'st_tz3_cpu_q1', 'st_tz3_cpu_median', 'st_tz3_cpu_q3', 'st_tz3_cpu_max', 'st_tz3_mem_min', 'st_tz3_mem_q1', 'st_tz3_mem_median', 'st_tz3_mem_q3', 'st_tz3_mem_max', 'short_term_tz4', 'st_tz4_cpu_min', 'st_tz4_cpu_q1', 'st_tz4_cpu_median', 'st_tz4_cpu_q3', 'st_tz4_cpu_max', 'st_tz4_mem_min', 'st_tz4_mem_q1', 'st_tz4_mem_median', 'st_tz4_mem_q3', 'st_tz4_mem_max', 'medium_term_tz1' , 'mt_tz1_cpu_min', 'mt_tz1_cpu_q1', 'mt_tz1_cpu_median', 'mt_tz1_cpu_q3', 'mt_tz1_cpu_max', 'mt_tz1_mem_min', 'mt_tz1_mem_q1', 'mt_tz1_mem_median', 'mt_tz1_mem_q3', 'mt_tz1_mem_max', 'medium_term_tz2', 'mt_tz2_cpu_min', 'mt_tz2_cpu_q1', 'mt_tz2_cpu_median', 'mt_tz2_cpu_q3', 'mt_tz2_cpu_max', 'mt_tz2_mem_min', 'mt_tz2_mem_q1', 'mt_tz2_mem_median', 'mt_tz2_mem_q3', 'mt_tz2_mem_max', 'medium_term_tz3', 'mt_tz3_cpu_min', 'mt_tz3_cpu_q1', 'mt_tz3_cpu_median', 'mt_tz3_cpu_q3', 'mt_tz3_cpu_max', 'mt_tz3_mem_min', 'mt_tz3_mem_q1', 'mt_tz3_mem_median', 'mt_tz3_mem_q3', 'mt_tz3_mem_max', 'medium_term_tz4', 'mt_tz4_cpu_min', 'mt_tz4_cpu_q1', 'mt_tz4_cpu_median', 'mt_tz4_cpu_q3', 'mt_tz4_cpu_max', 'mt_tz4_mem_min', 'mt_tz4_mem_q1', 'mt_tz4_mem_median', 'mt_tz4_mem_q3', 'mt_tz4_mem_max', 'medium_term_tz5', 'mt_tz5_cpu_min', 'mt_tz5_cpu_q1', 'mt_tz5_cpu_median', 'mt_tz5_cpu_q3', 'mt_tz5_cpu_max', 'mt_tz5_mem_min', 'mt_tz5_mem_q1', 'mt_tz5_mem_median', 'mt_tz5_mem_q3', 'mt_tz5_mem_max', 'medium_term_tz6', 'mt_tz6_cpu_min', 'mt_tz6_cpu_q1', 'mt_tz6_cpu_median', 'mt_tz6_cpu_q3', 'mt_tz6_cpu_max', 'mt_tz6_mem_min', 'mt_tz6_mem_q1', 'mt_tz6_mem_median', 'mt_tz6_mem_q3', 'mt_tz6_mem_max', 'medium_term_tz7', 'mt_tz7_cpu_min', 'mt_tz7_cpu_q1', 'mt_tz7_cpu_median', 'mt_tz7_cpu_q3', 'mt_tz7_cpu_max', 'mt_tz7_mem_min', 'mt_tz7_mem_q1', 'mt_tz7_mem_median', 'mt_tz7_mem_q3', 'mt_tz7_mem_max', 'long_term_tz1' , 'lt_tz1_cpu_min', 'lt_tz1_cpu_q1', 'lt_tz1_cpu_median', 'lt_tz1_cpu_q3', 'lt_tz1_cpu_max', 'lt_tz1_mem_min', 'lt_tz1_mem_q1', 'lt_tz1_mem_median', 'lt_tz1_mem_q3', 'lt_tz1_mem_max', 'long_term_tz2', 'lt_tz2_cpu_min', 'lt_tz2_cpu_q1', 'lt_tz2_cpu_median', 'lt_tz2_cpu_q3', 'lt_tz2_cpu_max', 'lt_tz2_mem_min', 'lt_tz2_mem_q1', 'lt_tz2_mem_median', 'lt_tz2_mem_q3', 'lt_tz2_mem_max', 'long_term_tz3', 'lt_tz3_cpu_min', 'lt_tz3_cpu_q1', 'lt_tz3_cpu_median', 'lt_tz3_cpu_q3', 'lt_tz3_cpu_max', 'lt_tz3_mem_min', 'lt_tz3_mem_q1', 'lt_tz3_mem_median', 'lt_tz3_mem_q3', 'lt_tz3_mem_max', 'long_term_tz4', 'lt_tz4_cpu_min', 'lt_tz4_cpu_q1', 'lt_tz4_cpu_median', 'lt_tz4_cpu_q3', 'lt_tz4_cpu_max', 'lt_tz4_mem_min', 'lt_tz4_mem_q1', 'lt_tz4_mem_median', 'lt_tz4_mem_q3', 'lt_tz4_mem_max', 'long_term_tz5', 'lt_tz5_cpu_min', 'lt_tz5_cpu_q1', 'lt_tz5_cpu_median', 'lt_tz5_cpu_q3', 'lt_tz5_cpu_max', 'lt_tz5_mem_min', 'lt_tz5_mem_q1', 'lt_tz5_mem_median', 'lt_tz5_mem_q3', 'lt_tz5_mem_max', 'long_term_tz6', 'lt_tz6_cpu_min', 'lt_tz6_cpu_q1', 'lt_tz6_cpu_median', 'lt_tz6_cpu_q3', 'lt_tz6_cpu_max', 'lt_tz6_mem_min', 'lt_tz6_mem_q1', 'lt_tz6_mem_median', 'lt_tz6_mem_q3', 'lt_tz6_mem_max', 'long_term_tz7', 'lt_tz7_cpu_min', 'lt_tz7_cpu_q1', 'lt_tz7_cpu_median', 'lt_tz7_cpu_q3', 'lt_tz7_cpu_max', 'lt_tz7_mem_min', 'lt_tz7_mem_q1', 'lt_tz7_mem_median', 'lt_tz7_mem_q3', 'lt_tz7_mem_max', 'long_term_tz8' , 'lt_tz8_cpu_min', 'lt_tz8_cpu_q1', 'lt_tz8_cpu_median', 'lt_tz8_cpu_q3', 'lt_tz8_cpu_max', 'lt_tz8_mem_min', 'lt_tz8_mem_q1', 'lt_tz8_mem_median', 'lt_tz8_mem_q3', 'lt_tz8_mem_max', 'long_term_tz9', 'lt_tz9_cpu_min', 'lt_tz9_cpu_q1', 'lt_tz9_cpu_median', 'lt_tz9_cpu_q3', 'lt_tz9_cpu_max', 'lt_tz9_mem_min', 'lt_tz9_mem_q1', 'lt_tz9_mem_median', 'lt_tz9_mem_q3', 'lt_tz9_mem_max', 'long_term_tz10', 'lt_tz10_cpu_min', 'lt_tz10_cpu_q1', 'lt_tz10_cpu_median', 'lt_tz10_cpu_q3', 'lt_tz10_cpu_max', 'lt_tz10_mem_min', 'lt_tz10_mem_q1', 'lt_tz10_mem_median', 'lt_tz10_mem_q3', 'lt_tz10_mem_max', 'long_term_tz11', 'lt_tz11_cpu_min', 'lt_tz11_cpu_q1', 'lt_tz11_cpu_median', 'lt_tz11_cpu_q3', 'lt_tz11_cpu_max', 'lt_tz11_mem_min', 'lt_tz11_mem_q1', 'lt_tz11_mem_median', 'lt_tz11_mem_q3', 'lt_tz11_mem_max', 'long_term_tz12', 'lt_tz12_cpu_min', 'lt_tz12_cpu_q1', 'lt_tz12_cpu_median', 'lt_tz12_cpu_q3', 'lt_tz12_cpu_max', 'lt_tz12_mem_min', 'lt_tz12_mem_q1', 'lt_tz12_mem_median', 'lt_tz12_mem_q3', 'lt_tz12_mem_max', 'long_term_tz13', 'lt_tz13_cpu_min', 'lt_tz13_cpu_q1', 'lt_tz13_cpu_median', 'lt_tz13_cpu_q3', 'lt_tz13_cpu_max', 'lt_tz13_mem_min', 'lt_tz13_mem_q1', 'lt_tz13_mem_median', 'lt_tz13_mem_q3', 'lt_tz13_mem_max', 'long_term_tz14', 'lt_tz14_cpu_min', 'lt_tz14_cpu_q1', 'lt_tz14_cpu_median', 'lt_tz14_cpu_q3', 'lt_tz14_cpu_max', 'lt_tz14_mem_min', 'lt_tz14_mem_q1', 'lt_tz14_mem_median', 'lt_tz14_mem_q3', 'lt_tz14_mem_max', 'long_term_tz15', 'lt_tz15_cpu_min', 'lt_tz15_cpu_q1', 'lt_tz15_cpu_median', 'lt_tz15_cpu_q3', 'lt_tz15_cpu_max', 'lt_tz15_mem_min', 'lt_tz15_mem_q1', 'lt_tz15_mem_median', 'lt_tz15_mem_q3', 'lt_tz15_mem_max']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            iexperiment_name = ""
            datadict = data[0]
            experiment_name = datadict["experiment_name"]
            for key, value in datadict.items():
                if key == "experiment_name":
                    print("experiment_name = ", value)
                    experiment_name = value
                if key == "cluster_name":
                    cluster_name = value
                if key == "kubernetes_objects":
                    for kobj in value:
                        containerNames=[]
                        k8ObjectName = ""
                        namespace = ""
                        k8ObjectType = ""
                        k8ObjectName = kobj["name"]
                        namespace = kobj["namespace"]
                        k8ObjectType = kobj["type"]
                        for containerName, container_data in kobj["containers"].items():
                            containerNames.append(containerName)
                            containerData = container_data.get("recommendations", {}).get("data", {})
                            if not containerData:
                                kobj_dict = {
                                'experiment_name': experiment_name,
                                'type': kobj["type"],
                                'name': kobj["name"],
                                'namespace': kobj["namespace"],
                                'container_name': container_data["container_name"],
                                }
                                writer.writerow(kobj_dict)
                            for timezone, timezone_data in containerData.items():
                                kobj_dict = {
                                'experiment_name': experiment_name,
                                'cluster_name': cluster_name,
                                'type': kobj["type"],
                                'name': kobj["name"],
                                'namespace': kobj["namespace"],
                                'container_name': container_data["container_name"],
                                'timezone': timezone,
                                }
                                plot_dict = {}
                                datapoint = 0

                                for recomm_type, recomm_typedata in timezone_data["recommendation_terms"].items():
                                    if "plots" in recomm_typedata:
                                        plotsData = recomm_typedata.get("plots", {}).get("plots_data", {})
                                        plot_datapoints = recomm_typedata["plots"]["datapoints"]
                                        datapoint = 0
                                        for plot_timezone, plot_timezone_data in plotsData.items():
                                            datapoint = datapoint + 1
                                            for plotresource_name, plotresource in plot_timezone_data.items():
                                                if plotresource_name == "cpuUsage":
                                                    plot_resource = 'cpu'
                                                if plotresource_name == "memoryUsage":
                                                    plot_resource = 'mem'

                                                if recomm_type == "short_term":
                                                    recomm_type_p = 'st'
                                                if recomm_type == "medium_term":
                                                    recomm_type_p = 'mt'
                                                if recomm_type == "long_term":
                                                    recomm_type_p = 'lt'

                                                plot_tz_var_name = recomm_type + '_tz' + str(datapoint)
                                                plot_min_var_name = recomm_type_p + '_tz' + str(datapoint) + '_' + plot_resource + '_min'
                                                plot_q1_var_name = recomm_type_p + '_tz' + str(datapoint) + '_' + plot_resource + '_q1'
                                                plot_median_var_name = recomm_type_p + '_tz' + str(datapoint) + '_' + plot_resource + '_median'
                                                plot_q3_var_name = recomm_type_p + '_tz' + str(datapoint) + '_' + plot_resource + '_q3'
                                                plot_max_var_name = recomm_type_p + '_tz' + str(datapoint) + '_' + plot_resource + '_max'

                                                plot_dict[plot_tz_var_name] = str(plot_timezone)
                                                plot_dict[plot_min_var_name] = str(plot_timezone_data[plotresource_name]["min"])
                                                plot_dict[plot_q1_var_name] = str(plot_timezone_data[plotresource_name]["q1"])
                                                plot_dict[plot_median_var_name] = str(plot_timezone_data[plotresource_name]["median"])
                                                plot_dict[plot_q3_var_name] = str(plot_timezone_data[plotresource_name]["q3"])
                                                plot_dict[plot_max_var_name] = str(plot_timezone_data[plotresource_name]["max"])

                                kobj_dict.update(plot_dict)
                                writer.writerow(kobj_dict)
        # Sort the data in chronological order of timezone
        with open('experimentPlotData_temp.csv', 'r') as csvfile:
            reader = csv.DictReader(csvfile)
            Edata = list(reader)
        data_sorted = sorted(Edata, key=lambda x: x['timezone'])

        with open('experimentPlotData_sorted.csv', 'w', newline='') as csvfile:
            fieldnames = reader.fieldnames
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data_sorted)

        # Write the sorted data back to the CSV file
        with open('experimentPlotData.csv', 'a', newline='') as csvfile:
            fieldnames = reader.fieldnames
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data_sorted)


def create_cluster_data_csv(csvtype,outputfile):
    if csvtype == "cluster":
        fieldnames = ['cluster_name', 'timezone', 'namespaces' , 'namespace_count', 'workloads', 'workload_count', 'short_term_current_cpu_requests', 'short_term_current_memory_requests', 'medium_term_current_cpu_requests', 'medium_term_current_memory_requests', 'long_term_current_cpu_requests', 'long_term_current_memory_requests', 'short_term_current_cpu_limits', 'short_term_current_memory_limits', 'medium_term_current_cpu_limits', 'medium_term_current_memory_limits', 'long_term_current_cpu_limits', 'long_term_current_memory_limits', 'cost_short_term_cpu_requests', 'cost_short_term_memory_requests', 'cost_short_term_cpu_limits', 'cost_short_term_memory_limits', 'cost_medium_term_cpu_requests', 'cost_medium_term_memory_requests', 'cost_medium_term_cpu_limits', 'cost_medium_term_memory_limits', 'cost_long_term_cpu_requests', 'cost_long_term_memory_requests', 'cost_long_term_cpu_limits', 'cost_long_term_memory_limits', 'cost_short_term_cpu_requests_variation', 'cost_short_term_memory_requests_variation', 'cost_short_term_cpu_limits_variation', 'cost_short_term_memory_limits_variation', 'cost_medium_term_cpu_requests_variation' , 'cost_medium_term_memory_requests_variation', 'cost_medium_term_cpu_limits_variation' , 'cost_medium_term_memory_limits_variation', 'cost_long_term_cpu_requests_variation' , 'cost_long_term_memory_requests_variation', 'cost_long_term_cpu_limits_variation' , 'cost_long_term_memory_limits_variation', 'cost_short_term_idle_cpu', 'cost_short_term_critical_cpu', 'cost_short_term_optimizable_cpu', 'cost_short_term_optimized_cpu', 'cost_short_term_error_cpu', 'cost_short_term_no_data_cpu', 'cost_short_term_total_cpu', 'cost_short_term_idle_memory', 'cost_short_term_critical_memory', 'cost_short_term_optimizable_memory', 'cost_short_term_optimized_memory', 'cost_short_term_error_memory', 'cost_short_term_no_data_memory', 'cost_short_term_total_memory', 'cost_short_term_error_general', 'cost_short_term_no_data_general', 'cost_short_term_total_general', 'cost_medium_term_idle_cpu', 'cost_medium_term_critical_cpu', 'cost_medium_term_optimizable_cpu', 'cost_medium_term_optimized_cpu', 'cost_medium_term_error_cpu', 'cost_medium_term_no_data_cpu', 'cost_medium_term_total_cpu', 'cost_medium_term_idle_memory', 'cost_medium_term_critical_memory', 'cost_medium_term_optimizable_memory', 'cost_medium_term_optimized_memory', 'cost_medium_term_error_memory', 'cost_medium_term_no_data_memory', 'cost_medium_term_total_memory', 'cost_medium_term_error_general', 'cost_medium_term_no_data_general', 'cost_medium_term_total_general', 'cost_long_term_idle_cpu', 'cost_long_term_critical_cpu', 'cost_long_term_optimizable_cpu', 'cost_long_term_optimized_cpu', 'cost_long_term_error_cpu', 'cost_long_term_no_data_cpu', 'cost_long_term_total_cpu', 'cost_long_term_idle_memory', 'cost_long_term_critical_memory', 'cost_long_term_optimizable_memory', 'cost_long_term_optimized_memory', 'cost_long_term_error_memory', 'cost_long_term_no_data_memory', 'cost_long_term_total_memory', 'cost_long_term_error_general', 'cost_long_term_no_data_general', 'cost_long_term_total_general', 'action_summary_no_data_general' ]
    elif csvtype == "clusterNamespace":
        fieldnames = ['namespace_name', 'timezone', 'clusters' , 'cluster_count' , 'workloads', 'workload_count', 'containers', 'container_count',  'short_term_current_cpu_requests', 'short_term_current_memory_requests', 'medium_term_current_cpu_requests', 'medium_term_current_memory_requests', 'long_term_current_cpu_requests', 'long_term_current_memory_requests', 'short_term_current_cpu_limits', 'short_term_current_memory_limits', 'medium_term_current_cpu_limits', 'medium_term_current_memory_limits', 'long_term_current_cpu_limits', 'long_term_current_memory_limits', 'cost_short_term_cpu_requests', 'cost_short_term_memory_requests', 'cost_short_term_cpu_limits', 'cost_short_term_memory_limits', 'cost_medium_term_cpu_requests', 'cost_medium_term_memory_requests', 'cost_medium_term_cpu_limits', 'cost_medium_term_memory_limits', 'cost_long_term_cpu_requests', 'cost_long_term_memory_requests', 'cost_long_term_cpu_limits', 'cost_long_term_memory_limits', 'cost_short_term_cpu_requests_variation', 'cost_short_term_memory_requests_variation', 'cost_short_term_cpu_limits_variation', 'cost_short_term_memory_limits_variation', 'cost_medium_term_cpu_requests_variation' , 'cost_medium_term_memory_requests_variation', 'cost_medium_term_cpu_limits_variation' , 'cost_medium_term_memory_limits_variation', 'cost_long_term_cpu_requests_variation' , 'cost_long_term_memory_requests_variation', 'cost_long_term_cpu_limits_variation' , 'cost_long_term_memory_limits_variation', 'cost_short_term_idle_cpu', 'cost_short_term_critical_cpu', 'cost_short_term_optimizable_cpu', 'cost_short_term_optimized_cpu', 'cost_short_term_error_cpu', 'cost_short_term_no_data_cpu', 'cost_short_term_total_cpu', 'cost_short_term_idle_memory', 'cost_short_term_critical_memory', 'cost_short_term_optimizable_memory', 'cost_short_term_optimized_memory', 'cost_short_term_error_memory', 'cost_short_term_no_data_memory', 'cost_short_term_total_memory', 'cost_short_term_error_general', 'cost_short_term_no_data_general', 'cost_short_term_total_general', 'cost_medium_term_idle_cpu', 'cost_medium_term_critical_cpu', 'cost_medium_term_optimizable_cpu', 'cost_medium_term_optimized_cpu', 'cost_medium_term_error_cpu', 'cost_medium_term_no_data_cpu', 'cost_medium_term_total_cpu', 'cost_medium_term_idle_memory', 'cost_medium_term_critical_memory', 'cost_medium_term_optimizable_memory', 'cost_medium_term_optimized_memory', 'cost_medium_term_error_memory', 'cost_medium_term_no_data_memory', 'cost_medium_term_total_memory', 'cost_medium_term_error_general', 'cost_medium_term_no_data_general', 'cost_medium_term_total_general', 'cost_long_term_idle_cpu', 'cost_long_term_critical_cpu', 'cost_long_term_optimizable_cpu', 'cost_long_term_optimized_cpu', 'cost_long_term_error_cpu', 'cost_long_term_no_data_cpu', 'cost_long_term_total_cpu', 'cost_long_term_idle_memory', 'cost_long_term_critical_memory', 'cost_long_term_optimizable_memory', 'cost_long_term_optimized_memory', 'cost_long_term_error_memory', 'cost_long_term_no_data_memory', 'cost_long_term_total_memory', 'cost_long_term_error_general', 'cost_long_term_no_data_general', 'cost_long_term_total_general', 'action_summary_no_data_general' ]
    with open(outputfile, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()



def get_cluster_data_csv(csvtype, filename, outputfile):
    with open(filename, 'r') as f:
        data = json.load(f)
    if csvtype == "cluster":
        fieldnames = ['cluster_name', 'timezone', 'namespaces' , 'namespace_count', 'workloads', 'workload_count', 'short_term_current_cpu_requests', 'short_term_current_memory_requests', 'medium_term_current_cpu_requests', 'medium_term_current_memory_requests', 'long_term_current_cpu_requests', 'long_term_current_memory_requests', 'short_term_current_cpu_limits', 'short_term_current_memory_limits', 'medium_term_current_cpu_limits', 'medium_term_current_memory_limits', 'long_term_current_cpu_limits', 'long_term_current_memory_limits', 'cost_short_term_cpu_requests', 'cost_short_term_memory_requests', 'cost_short_term_cpu_limits', 'cost_short_term_memory_limits', 'cost_medium_term_cpu_requests', 'cost_medium_term_memory_requests', 'cost_medium_term_cpu_limits', 'cost_medium_term_memory_limits', 'cost_long_term_cpu_requests', 'cost_long_term_memory_requests', 'cost_long_term_cpu_limits', 'cost_long_term_memory_limits', 'cost_short_term_cpu_requests_variation', 'cost_short_term_memory_requests_variation', 'cost_short_term_cpu_limits_variation', 'cost_short_term_memory_limits_variation', 'cost_medium_term_cpu_requests_variation' , 'cost_medium_term_memory_requests_variation', 'cost_medium_term_cpu_limits_variation' , 'cost_medium_term_memory_limits_variation', 'cost_long_term_cpu_requests_variation' , 'cost_long_term_memory_requests_variation', 'cost_long_term_cpu_limits_variation' , 'cost_long_term_memory_limits_variation', 'cost_short_term_idle_cpu', 'cost_short_term_critical_cpu', 'cost_short_term_optimizable_cpu', 'cost_short_term_optimized_cpu', 'cost_short_term_error_cpu', 'cost_short_term_no_data_cpu', 'cost_short_term_total_cpu', 'cost_short_term_idle_memory', 'cost_short_term_critical_memory', 'cost_short_term_optimizable_memory', 'cost_short_term_optimized_memory', 'cost_short_term_error_memory', 'cost_short_term_no_data_memory', 'cost_short_term_total_memory', 'cost_short_term_error_general', 'cost_short_term_no_data_general', 'cost_short_term_total_general', 'cost_medium_term_idle_cpu', 'cost_medium_term_critical_cpu', 'cost_medium_term_optimizable_cpu', 'cost_medium_term_optimized_cpu', 'cost_medium_term_error_cpu', 'cost_medium_term_no_data_cpu', 'cost_medium_term_total_cpu', 'cost_medium_term_idle_memory', 'cost_medium_term_critical_memory', 'cost_medium_term_optimizable_memory', 'cost_medium_term_optimized_memory', 'cost_medium_term_error_memory', 'cost_medium_term_no_data_memory', 'cost_medium_term_total_memory', 'cost_medium_term_error_general', 'cost_medium_term_no_data_general', 'cost_medium_term_total_general', 'cost_long_term_idle_cpu', 'cost_long_term_critical_cpu', 'cost_long_term_optimizable_cpu', 'cost_long_term_optimized_cpu', 'cost_long_term_error_cpu', 'cost_long_term_no_data_cpu', 'cost_long_term_total_cpu', 'cost_long_term_idle_memory', 'cost_long_term_critical_memory', 'cost_long_term_optimizable_memory', 'cost_long_term_optimized_memory', 'cost_long_term_error_memory', 'cost_long_term_no_data_memory', 'cost_long_term_total_memory', 'cost_long_term_error_general', 'cost_long_term_no_data_general', 'cost_long_term_total_general', 'action_summary_no_data_general' ]
    elif csvtype == "clusterNamespace":
        fieldnames = ['namespace_name', 'timezone', 'clusters' , 'cluster_count' , 'workloads', 'workload_count', 'containers', 'container_count',  'short_term_current_cpu_requests', 'short_term_current_memory_requests', 'medium_term_current_cpu_requests', 'medium_term_current_memory_requests', 'long_term_current_cpu_requests', 'long_term_current_memory_requests', 'short_term_current_cpu_limits', 'short_term_current_memory_limits', 'medium_term_current_cpu_limits', 'medium_term_current_memory_limits', 'long_term_current_cpu_limits', 'long_term_current_memory_limits', 'cost_short_term_cpu_requests', 'cost_short_term_memory_requests', 'cost_short_term_cpu_limits', 'cost_short_term_memory_limits', 'cost_medium_term_cpu_requests', 'cost_medium_term_memory_requests', 'cost_medium_term_cpu_limits', 'cost_medium_term_memory_limits', 'cost_long_term_cpu_requests', 'cost_long_term_memory_requests', 'cost_long_term_cpu_limits', 'cost_long_term_memory_limits', 'cost_short_term_cpu_requests_variation', 'cost_short_term_memory_requests_variation', 'cost_short_term_cpu_limits_variation', 'cost_short_term_memory_limits_variation', 'cost_medium_term_cpu_requests_variation' , 'cost_medium_term_memory_requests_variation', 'cost_medium_term_cpu_limits_variation' , 'cost_medium_term_memory_limits_variation', 'cost_long_term_cpu_requests_variation' , 'cost_long_term_memory_requests_variation', 'cost_long_term_cpu_limits_variation' , 'cost_long_term_memory_limits_variation', 'cost_short_term_idle_cpu', 'cost_short_term_critical_cpu', 'cost_short_term_optimizable_cpu', 'cost_short_term_optimized_cpu', 'cost_short_term_error_cpu', 'cost_short_term_no_data_cpu', 'cost_short_term_total_cpu', 'cost_short_term_idle_memory', 'cost_short_term_critical_memory', 'cost_short_term_optimizable_memory', 'cost_short_term_optimized_memory', 'cost_short_term_error_memory', 'cost_short_term_no_data_memory', 'cost_short_term_total_memory', 'cost_short_term_error_general', 'cost_short_term_no_data_general', 'cost_short_term_total_general', 'cost_medium_term_idle_cpu', 'cost_medium_term_critical_cpu', 'cost_medium_term_optimizable_cpu', 'cost_medium_term_optimized_cpu', 'cost_medium_term_error_cpu', 'cost_medium_term_no_data_cpu', 'cost_medium_term_total_cpu', 'cost_medium_term_idle_memory', 'cost_medium_term_critical_memory', 'cost_medium_term_optimizable_memory', 'cost_medium_term_optimized_memory', 'cost_medium_term_error_memory', 'cost_medium_term_no_data_memory', 'cost_medium_term_total_memory', 'cost_medium_term_error_general', 'cost_medium_term_no_data_general', 'cost_medium_term_total_general', 'cost_long_term_idle_cpu', 'cost_long_term_critical_cpu', 'cost_long_term_optimizable_cpu', 'cost_long_term_optimized_cpu', 'cost_long_term_error_cpu', 'cost_long_term_no_data_cpu', 'cost_long_term_total_cpu', 'cost_long_term_idle_memory', 'cost_long_term_critical_memory', 'cost_long_term_optimizable_memory', 'cost_long_term_optimized_memory', 'cost_long_term_error_memory', 'cost_long_term_no_data_memory', 'cost_long_term_total_memory', 'cost_long_term_error_general', 'cost_long_term_no_data_general', 'cost_long_term_total_general', 'action_summary_no_data_general' ]
    with open(outputfile, 'a', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        for cluster in data:
            if not cluster:
                continue
            if csvtype == "cluster":
                cluster_name = cluster['cluster_name']
                namespaces = ', '.join(cluster['namespaces']['names'])
                namespace_count = cluster['namespaces']['count']
                workloads = ', '.join(cluster['workloads']['names'])
                workload_count = cluster['workloads']['count']
            elif csvtype == "clusterNamespace":
                namespace_name = cluster['namespace']
                clusters = ''
                cluster_count = ''
                if 'clusters' in cluster:
                    clusters = ', '.join(cluster['clusters']['names'])
                    cluster_count = cluster['clusters']['count']
                elif 'cluster_name' in cluster:
                    clusters = cluster['cluster_name']
                workloads = ''
                workload_count = ''
                if 'workloads' in cluster:
                    workloads = ', '.join(cluster['workloads']['names'])
                    workload_count = cluster['workloads']['count']

                containers = ''
                container_count = ''
                if 'containers' in cluster:
                    containers = ', '.join(cluster['containers']['names'])
                    container_count = cluster['containers']['count']
            action_summary_dict = {}
            for action_type, action_data in cluster['action_summary'].items():
                for action_resource, action_value in action_data.items():
                    action_var_name = 'action_summary_' + action_type + '_' + action_resource
                    action_summary_dict[action_var_name] = str(action_value["count"])
            for date, metrics in cluster['summary']['data'].items():
                if csvtype == "clusterNamespace":
                    clust_dict = {
                            'namespace_name' : namespace_name,
                            'timezone': date,
                            'clusters': clusters,
                            'cluster_count': cluster_count,
                            'workloads': workloads,
                            'workload_count': workload_count,
                            'containers': containers,
                            'container_count': container_count
                            }
                elif csvtype == "cluster":
                    clust_dict = {
                            'cluster_name': cluster_name,
                            'timezone': date,
                            'namespaces': namespaces,
                            'namespace_count': namespace_count,
                            'workloads': workloads,
                            'workload_count': workload_count
                            }
                recomm_dict = {}
                for recomm_engine, recomm_enginedata in metrics.items():
                    for recomm_type, recomm_typedata in recomm_enginedata.items():
                        if "current" in recomm_typedata:
                            for recomm_current, recomm_currentmetrics in recomm_typedata["current"].items():
                                for recomm_resource, recomm_resourcedata in recomm_currentmetrics.items():
                                    recomm_var_name = recomm_type + '_current_' + recomm_resource + '_' + recomm_current
                                    recomm_dict[recomm_var_name] = str(recomm_resourcedata["amount"])
                        if "config" in recomm_typedata:
                            for recomm_config, recomm_configmetrics in recomm_typedata["config"].items():
                                for recomm_resource, recomm_resourcedata in recomm_configmetrics.items():
                                    recomm_var_name = recomm_engine + '_' + recomm_type + '_' + recomm_resource + '_' + recomm_config
                                    #recomm_var_format = recomm_engine + '_' + recomm_type + '_' + recomm_resource + '_' + recomm_config + '_format'
                                    recomm_dict[recomm_var_name] = str(recomm_resourcedata["amount"])
                                    #recomm_dict[recomm_var_format] = str(recomm_resourcedata["format"])
                        if "change" in recomm_typedata:
                            for recomm_config, recomm_configmetrics in recomm_typedata["change"]["variation"].items():
                                for recomm_resource, recomm_resourcedata in recomm_configmetrics.items():
                                    recomm_var_name = recomm_engine + '_' + recomm_type + '_' + recomm_resource + '_' + recomm_config + '_variation'
                                    recomm_dict[recomm_var_name] = str(recomm_resourcedata["amount"])
                        if "action_summary" in recomm_typedata:
                            for action_type, action_data in recomm_typedata["action_summary"].items():
                                for action_resource, action_value in action_data.items():
                                    action_var_name = recomm_engine + '_' + recomm_type + '_' + action_type + '_' + action_resource
                                    recomm_dict[action_var_name] = str(action_value["count"])
                clust_dict.update(recomm_dict)
                clust_dict.update(action_summary_dict)
                writer.writerow(clust_dict)

def get_value_fromcsv(filename, recommendation_type):
    with open(filename, 'r') as file:
        reader = csv.DictReader(file)
        for row in reader:
            value = row[recommendation_type]
        return value

def setRecommendations(filename,recommendation_type):
    get_recommondations(filename)
    cpu_request_value = ''
    cpu_limit_value = ''
    memory_request_value = ''
    memory_limit_value = ''
    if recommendation_type == "short_term":
        cpu_request_type = "cost_short_term_cpu_requests"
        cpu_limit_type = "cost_short_term_cpu_limits"
        mem_request_type = "cost_short_term_memory_requests"
        mem_limit_type = "cost_short_term_memory_limits"
        cpu_format = "cost_short_term_cpu_requests_format"
        mem_format = "cost_short_term_memory_requests_format"
    elif recommendation_type == "medium_term":
        cpu_request_type = "cost_medium_term_cpu_requests"
        cpu_limit_type = "cost_medium_term_cpu_limits"
        mem_request_type = "cost_medium_term_memory_requests"
        mem_limit_type = "cost_medium_term_memory_limits"
        cpu_format = "cost_medium_term_cpu_requests_format"
        mem_format = "cost_medium_term_memory_requests_format"
    elif recommendation_type == "long_term":
        cpu_request_type = "cost_long_term_cpu_requests"
        cpu_limit_type = "cost_long_term_cpu_limits"
        mem_request_type = "cost_long_term_memory_requests"
        mem_limit_type = "cost_long_term_memory_limits"
        cpu_format = "cost_long_term_cpu_requests_format"
        mem_format = "cost_long_term_memory_requests_format"

    cpu_request_value=get_value_fromcsv('experimentRecommendations.csv', cpu_request_type)
    cpu_limit_value=get_value_fromcsv('experimentRecommendations.csv', cpu_limit_type)
    mem_format=get_value_fromcsv('experimentRecommendations.csv', mem_format)
    mem_request_from_csv = get_value_fromcsv('experimentRecommendations.csv', mem_request_type)
    mem_limit_from_csv = get_value_fromcsv('experimentRecommendations.csv', mem_limit_type)
    if mem_format == "GiB":
        if mem_request_from_csv != '':
            memory_request_value=str(round(float(get_value_fromcsv('experimentRecommendations.csv', mem_request_type))))+"Gi"
        if mem_limit_from_csv != '':
            memory_limit_value=str(round(float(get_value_fromcsv('experimentRecommendations.csv', mem_limit_type))))+"Gi"
    elif mem_format == "MiB":
        if mem_request_from_csv != '':
            memory_request_value=str(round(float(get_value_fromcsv('experimentRecommendations.csv', mem_request_type))))+"Mi"
        if mem_limit_from_csv != '':
            memory_limit_value=str(round(float(get_value_fromcsv('experimentRecommendations.csv', mem_limit_type))))+"Mi"
    else:
        if mem_request_from_csv != '':
            memory_request_value=str(round(float(get_value_fromcsv('experimentRecommendations.csv', mem_request_type))))
        if mem_limit_from_csv != '':
            memory_limit_value=str(round(float(get_value_fromcsv('experimentRecommendations.csv', mem_limit_type))))

    kubernetes_object_type=get_value_fromcsv('experimentRecommendations.csv', 'type')
    kubernetes_object_name=get_value_fromcsv('experimentRecommendations.csv', 'name')
    kubernetes_object_namespace=get_value_fromcsv('experimentRecommendations.csv', 'namespace')
    kubernetes_object_container=get_value_fromcsv('experimentRecommendations.csv', 'container_name')

    requests_str = ""
    limits_str = ""

    if cpu_request_value != '':
        requests_str += f"cpu={cpu_request_value},"
    if memory_request_value != '':
        requests_str += f"memory={memory_request_value},"

    if cpu_limit_value != '':
        limits_str += f"cpu={cpu_limit_value},"
    if memory_limit_value != '':
        limits_str += f"memory={memory_limit_value},"

    command = f"kubectl -n {kubernetes_object_namespace} set resources {kubernetes_object_type}/{kubernetes_object_name} --containers={kubernetes_object_container}"

    # Append the requests and limits parts only if they are not empty
    if requests_str != '':
        requests_str = requests_str.rstrip(',')
        command += f" --requests='{requests_str}'"
    if limits_str != '':
        limits_str = limits_str.rstrip(',')
        command += f" --limits='{limits_str}'"
    print("Applying recommendations using following command..")
    print(command)

    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = process.communicate()

# Function to add columns(like cluster_name) to the csv's
def add_column_to_csv(folder_path, column_name, column_value, start_index):
    source_dirs = sorted([d for d in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, d))])
    for cluster_index, source_dir in enumerate(source_dirs, start=start_index):
        source_path = os.path.join(folder_path, source_dir)
        if not os.path.isdir(source_path):
            continue
        date_dirs = sorted([d for d in os.listdir(source_path) if os.path.isdir(os.path.join(source_path, d))])
        for date_dir in date_dirs:
            date_path = os.path.join(source_path, date_dir)
            csv_files = [f for f in os.listdir(date_path) if f.endswith('.csv')]
            for csv_file in csv_files:
                csv_path = os.path.join(date_path, csv_file)
                df = pd.read_csv(csv_path)
                new_column_value = f'{column_value}{cluster_index}'
                df[column_name] = new_column_value
                df.to_csv(csv_path, index=False)
